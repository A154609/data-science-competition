{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["!pip install cnn_finetune"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import pandas as pd\n","train_label=pd.read_csv('../input/fu-data/data/train_label.csv')\n","len(train_label[train_label['label']==1.0]),len(train_label[train_label['label']==0.0]) #可见正负样本十分均衡"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class Config(object):\n","    backbone = 'xception'#\n","    num_classes = 2 #\n","    use_smooth_label=False\n","    loss = 'CrossEntropyLoss'#focal_loss/CrossEntropyLoss\n","    input_size = 384\n","    train_batch_size = 16  # batch size\n","    val_batch_size = 12\n","    test_batch_size = 1\n","    optimizer = 'adam'#sam/adam\n","    lr_scheduler='exp'#cosine/exp/poly\n","    lr = 3e-4  # adam 0.00001\n","    sam_lr=1e-3\n","    MOMENTUM = 0.9\n","    device = \"cuda\"  # cuda  or cpu\n","    gpu_id = [0]\n","    num_workers = 8  # how many workers for loading data\n","    max_epoch = 21\n","    weight_decay = 5e-4\n","    val_interval = 1\n","    print_interval = 50\n","    save_interval = 2\n","    tensorboard_interval=50\n","    min_save_epoch=1\n","    load_from = None\n","    #\n","    log_dir = 'log/'\n","    train_val_data = '../input/fu-data/data/train/'\n","    train_label_csv = '../input/fu-data/data/train_label.csv'\n","    #\n","    checkpoints_dir = './ckpt/'\n","    pre_trained = '..'"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import os\n","import glob\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset,DataLoader\n","import numpy as np\n","from torchvision import transforms as T\n","import torchvision\n","import cv2\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","class fuDataset(Dataset):\n","    def __init__(self, root, train_label_csv, phase='train', input_size=224):\n","        self.phase = phase\n","        train_val_label=pd.read_csv('../input/fu-data/data/train_label.csv')\n","        val_ids=[i for i in range(len(train_label)) if i%5==0]#验证集\n","        train_ids=[i for i in range(len(train_label)) if i%5!=0]#训练集\n","        if phase=='train':\n","            img_label=train_val_label[train_val_label.index.isin(train_ids)].reset_index()\n","            self.img_names=[os.path.join(root,i) for i in img_label['img_id'].values]\n","            self.labels=img_label['label'].values\n","        else:\n","            img_label=train_val_label[train_val_label.index.isin(val_ids)].reset_index()\n","            self.img_names=[os.path.join(root,i) for i in img_label['img_id'].values]\n","            self.labels=img_label['label'].values\n","        #使用全部数据训练（不要验证集）\n","        self.img_names=[os.path.join(root,i) for i in train_val_label['img_id'].values]\n","        self.labels=train_val_label['label'].values\n","        #\n","        normalize = T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","        if self.phase == 'train':\n","            self.transforms = T.Compose([\n","                T.Resize((input_size,input_size)),\n","                T.RandomHorizontalFlip(p=0.5),\n","                T.RandomVerticalFlip(p=0.25),\n","                T.RandomRotation(degrees=(-20,20)),\n","                T.ColorJitter(0.2,0.2),\n","                T.ToTensor(),\n","                normalize\n","            ])\n","        else:\n","            self.transforms = T.Compose([\n","                T.Resize((input_size,input_size)),\n","                T.ToTensor(),\n","                normalize\n","            ])\n","\n","    def __getitem__(self, index):\n","        img_path = self.img_names[index]\n","        data = Image.open(img_path)\n","        data = data.convert('RGB')\n","        data = self.transforms(data)\n","        label = np.int32(self.labels[index])\n","        return data.float(), label\n","\n","    def __len__(self):\n","        return len(self.img_names)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import logging\n","\n","def get_logger(filename, verbosity=1, name=None):\n","    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n","    formatter = logging.Formatter(\n","        \"[%(asctime)s][%(filename)s][%(levelname)s] %(message)s\"\n","    )\n","    logger = logging.getLogger(name)\n","    logger.setLevel(level_dict[verbosity])\n","\n","    fh = logging.FileHandler(filename, \"w\")\n","    fh.setFormatter(formatter)\n","    logger.addHandler(fh)\n","\n","    sh = logging.StreamHandler()\n","    sh.setFormatter(formatter)\n","    logger.addHandler(sh)\n","    return logger\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import time\n","from sklearn.metrics import accuracy_score\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","import matplotlib.pyplot as plt\n","from tensorboardX import SummaryWriter\n","import numpy as np\n","from cnn_finetune import make_model"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":["#\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","def train_model(model,criterion, optimizer, lr_scheduler=None):\n","\n","    train_dataset = fuDataset(opt.train_val_data, opt.train_label_csv, phase='train', input_size=opt.input_size)\n","    trainloader = DataLoader(train_dataset,\n","                             batch_size=opt.train_batch_size,\n","                             shuffle=True,\n","                             num_workers=opt.num_workers)\n","\n","    total_iters=len(trainloader)\n","    logger.info('total_iters:{}'.format(total_iters))\n","    model_name=opt.backbone\n","    since = time.time()\n","    best_score = 0.0\n","    best_epoch = 0\n","    log_acc=0\n","    log_train=0\n","    writer = SummaryWriter()  # 用于记录训练和测试的信息:loss,acc等\n","    logger.info('start training...')\n","    #\n","    iters = len(trainloader)\n","    for epoch in range(1,opt.max_epoch+1):\n","        model.train(True)\n","        begin_time=time.time()\n","        logger.info('learning rate:{}'.format(optimizer.param_groups[-1]['lr']))\n","        logger.info('Epoch {}/{}'.format(epoch, opt.max_epoch))\n","        logger.info('-' * 10)\n","        running_corrects_linear = 0\n","        count=0\n","        train_loss = []\n","        for i, data in enumerate(trainloader):\n","            count+=1\n","            inputs, labels = data\n","            labels = labels.type(torch.LongTensor)\n","            inputs, labels = inputs.cuda(), labels.cuda()\n","            #\n","            out_linear= model(inputs)\n","            _, linear_preds = torch.max(out_linear.data, 1)\n","            loss = criterion(out_linear, labels)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # 更新cosine学习率\n","            lr_scheduler.step(epoch + count / iters)\n","\n","            if i % opt.print_interval == 0 or out_linear.size()[0] < opt.train_batch_size:\n","                spend_time = time.time() - begin_time\n","                logger.info(\n","                    ' Epoch:{}({}/{}) loss:{:.3f} lr:{:.7f} epoch_Time:{}min:'.format(\n","                        epoch, count, total_iters,\n","                        loss.item(), optimizer.param_groups[-1]['lr'],\n","                        spend_time / count * total_iters // 60 - spend_time // 60))\n","            #\n","            running_corrects_linear += torch.sum(linear_preds == labels.data)\n","            train_loss.append(loss.item())\n","            writer.add_scalar('train_loss',loss.item(), global_step=log_train)\n","            log_train+=1\n","            #\n","        #lr_scheduler.step()\n","        val_acc,val_loss= val_model(model, criterion)\n","        epoch_acc_linear = running_corrects_linear.double() / total_iters / opt.train_batch_size\n","        logger.info('valLoss: {:.4f} valAcc: {:.4f}'.format(val_loss,val_acc))\n","        logger.info('Epoch:[{}/{}] train_acc={:.3f} '.format(epoch, opt.max_epoch,\n","                                                                    epoch_acc_linear))\n","        #\n","        model_out_path = model_save_dir + \"/\" + '{}_'.format(model_name) + str(epoch) + '.pth'\n","        best_model_out_path = model_save_dir + \"/\" + '{}_'.format(model_name) + 'best' + '.pth'\n","        #model_out_path = '{}_'.format(model_name) + str(epoch) + '.pth'\n","        #save the best model\n","        if val_acc > best_score:\n","            best_score = val_acc\n","            best_epoch=epoch\n","            torch.save(model.state_dict(), best_model_out_path)\n","            logger.info(\"save best epoch: {} best acc: {}\".format(best_epoch,val_acc))\n","        #save based on epoch interval\n","        if epoch % opt.save_interval == 0 and epoch>opt.min_save_epoch:\n","            torch.save(model.state_dict(), model_out_path)\n","    #\n","    logger.info('Best acc: {:.3f} Best epoch:{}'.format(best_score,best_epoch))\n","    time_elapsed = time.time() - since\n","    logger.info('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    writer.close()\n","\n","@torch.no_grad()\n","def val_model(model, criterion):\n","    val_dataset = fuDataset(opt.train_val_data, opt.train_label_csv, phase='val', input_size=opt.input_size)\n","    val_loader = DataLoader(val_dataset,\n","                             batch_size=opt.val_batch_size,\n","                             shuffle=False,\n","                             num_workers=opt.num_workers)\n","    dset_sizes=len(val_dataset)\n","    model.eval()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    cont = 0\n","    outPre = []\n","    outLabel = []\n","    pres_list=[]\n","    labels_list=[]\n","    for data in val_loader:\n","        inputs, labels = data\n","        labels = labels.type(torch.LongTensor)\n","        inputs, labels = inputs.cuda(), labels.cuda()\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs.data, 1)\n","        loss = criterion(outputs, labels)\n","        if cont == 0:\n","            outPre = outputs.data.cpu()\n","            outLabel = labels.data.cpu()\n","        else:\n","            outPre = torch.cat((outPre, outputs.data.cpu()), 0)\n","            outLabel = torch.cat((outLabel, labels.data.cpu()), 0)\n","        pres_list+=preds.cpu().numpy().tolist()\n","        labels_list+=labels.data.cpu().numpy().tolist()\n","        running_loss += loss.item() * inputs.size(0)\n","        running_corrects += torch.sum(preds == labels.data)\n","        cont += 1\n","    #\n","    val_acc = accuracy_score(labels_list, pres_list)\n","    return val_acc,running_loss / dset_sizes\n","\n","#\n","if __name__ == \"__main__\":\n","    #\n","    opt = Config()\n","    torch.cuda.empty_cache()\n","    device = torch.device(opt.device)\n","    criterion = torch.nn.CrossEntropyLoss().cuda()\n","    model_name=opt.backbone\n","    model_save_dir =os.path.join(opt.checkpoints_dir , model_name)\n","    if not os.path.exists(model_save_dir): os.makedirs(model_save_dir)\n","    logger = get_logger(os.path.join(model_save_dir,'log.log'))\n","    logger.info('Using: {}'.format(model_name))\n","    logger.info('InputSize: {}'.format(opt.input_size))\n","    logger.info('optimizer: {}'.format(opt.optimizer))\n","    logger.info('lr_init: {}'.format(opt.lr))\n","    logger.info('batch size: {}'.format(opt.train_batch_size))\n","    logger.info('criterion: {}'.format(opt.loss))\n","    logger.info('Using label smooth: {}'.format(opt.use_smooth_label))\n","    logger.info('lr_scheduler: {}'.format(opt.lr_scheduler))\n","    logger.info('Using the GPU: {}'.format(str(opt.gpu_id)))\n","\n","    model  = make_model('{}'.format('xception'), num_classes=2,\n","                        pretrained=True)\n","    model.to(device)\n","    optimizer = optim.AdamW(model.parameters(), lr=3e-4 ,weight_decay=5e-4)\n","    #lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n","    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=3, T_mult=2, eta_min=1e-6, last_epoch=-1)\n","    train_model(model, criterion, optimizer,\n","              lr_scheduler=lr_scheduler)\n","    #"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["submit=pd.read_csv('../input/fu-data/data/result.csv',header=None)\n","submit.columns=['name']\n","model  = make_model('{}'.format('xception'), num_classes=2,\n","                        pretrained=False)\n","net_weight='./ckpt/xception/xception_20.pth'\n","model.load_state_dict(torch.load(net_weight))\n","model = model.cuda()\n","model.eval()\n","#\n","infer_transforms=T.Compose([\n","                T.Resize((opt.input_size,opt.input_size)),\n","                T.ToTensor(),\n","                T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","            ])\n","result=[]\n","test_dir='../input/fu-data/data/test/'\n","for name in submit['name'].values:\n","    img_path=os.path.join(test_dir,name)\n","    data = Image.open(img_path)\n","    data = data.convert('RGB')\n","    data = infer_transforms(data)\n","    data=data.unsqueeze(0)\n","    inputs= data.cuda()\n","    with torch.no_grad():\n","        outputs = model(inputs)\n","    _, preds = torch.max(outputs.data, 1)\n","    result.append(preds.cpu().data.numpy()[0])\n","    #\n","submit['label']=result\n","submit.to_csv('submit.csv',index=False,header=None)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["submit"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}